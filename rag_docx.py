import os
import faiss
import requests
import pickle
import hashlib
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
DOCS_DIR = "docs"  # –ü–∞–ø–∫–∞ —Å .docx —Ñ–∞–π–ª–∞–º–∏
INDEX_FILE = "faiss_index.bin"
CHUNKS_FILE = "chunks.pkl"
HASH_FILE = "doc_hashes.pkl"
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "llama3:8b"

# === –•—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ (–¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π) ===
def file_hash(path):
    h = hashlib.md5()
    with open(path, "rb") as f:
        h.update(f.read())
    return h.hexdigest()

# === –ß—Ç–µ–Ω–∏–µ .docx ===
def read_docx(path):
    doc = Document(path)
    return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])

# === –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ ===
def split_text(text, chunk_size=500, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = min(len(text), start + chunk_size)
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

# === –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ ===
class DocIndex:
    def __init__(self):
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        self.embed_model = SentenceTransformer("all-MiniLM-L6-v2")
        self.index = None
        self.chunks = []
        self.doc_hashes = {}

    def add_docs_from_folder(self, folder):
        new_chunks = []
        updated = False

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—à–ª—ã–µ —Ö—ç—à–∏
        if os.path.exists(HASH_FILE):
            with open(HASH_FILE, "rb") as f:
                self.doc_hashes = pickle.load(f)

        for file in os.listdir(folder):
            if file.lower().endswith(".docx"):
                path = os.path.join(folder, file)
                h = file_hash(path)

                # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç –≤ —Ö—ç—à–∞—Ö –∏–ª–∏ –æ–Ω –∏–∑–º–µ–Ω–∏–ª—Å—è
                if file not in self.doc_hashes or self.doc_hashes[file] != h:
                    print(f"üìÑ –û–±–Ω–æ–≤–ª—è–µ–º: {file}")
                    text = read_docx(path)
                    doc_chunks = split_text(text)
                    new_chunks.extend(doc_chunks)
                    self.doc_hashes[file] = h
                    updated = True

        # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º –µ–≥–æ
        if os.path.exists(INDEX_FILE) and os.path.exists(CHUNKS_FILE) and not updated:
            print("‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å...")
            with open(CHUNKS_FILE, "rb") as f:
                self.chunks = pickle.load(f)
            self.index = faiss.read_index(INDEX_FILE)
            return

        # –ï—Å–ª–∏ –±—ã–ª–∏ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        if updated or not os.path.exists(INDEX_FILE):
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if os.path.exists(CHUNKS_FILE):
                with open(CHUNKS_FILE, "rb") as f:
                    old_chunks = pickle.load(f)
                self.chunks = old_chunks + new_chunks
            else:
                self.chunks = new_chunks

            print("üìä –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å...")
            vectors = self.embed_model.encode(self.chunks, show_progress_bar=True)
            vectors = np.array(vectors, dtype="float32")
            self.index = faiss.IndexFlatL2(vectors.shape[1])
            self.index.add(vectors)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë
            faiss.write_index(self.index, INDEX_FILE)
            with open(CHUNKS_FILE, "wb") as f:
                pickle.dump(self.chunks, f)
            with open(HASH_FILE, "wb") as f:
                pickle.dump(self.doc_hashes, f)

    def search(self, query, top_k=3):
        q_vec = self.embed_model.encode([query])
        q_vec = np.array(q_vec, dtype="float32")
        D, I = self.index.search(q_vec, top_k)
        return [self.chunks[i] for i in I[0]]

# === –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Ollama ===
def ask_ollama(prompt):
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": True
    }
    response = requests.post(OLLAMA_URL, json=payload, stream=True)
    answer = ""
    for line in response.iter_lines():
        if line:
            try:
                data = line.decode("utf-8")
                if '"response":"' in data:
                    part = data.split('"response":"')[1].split('"')[0]
                    answer += part
                    print(part, end="", flush=True)
            except:
                pass
    print()
    return answer

# === –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ ===
if __name__ == "__main__":
    index = DocIndex()
    index.add_docs_from_folder(DOCS_DIR)

    while True:
        q = input("\n–í–æ–ø—Ä–æ—Å: ")
        if q.lower() == "exit":
            break
        relevant_chunks = index.search(q, top_k=3)
        context = "\n\n".join(relevant_chunks)
        prompt = f"–ò—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ, –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {q}\n–û—Ç–≤–µ—Ç:"
        ask_ollama(prompt)
